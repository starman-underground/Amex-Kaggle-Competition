{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6609204e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This notebook demonstrates an advanced approach to the AMEX Default Prediction competition with:\n",
    "\n",
    "- GPU Acceleration: Uses RAPIDS (cuDF, cuPy) for fast data processing\n",
    "\n",
    "- Memory Efficiency: Implements custom data iterators and chunking strategies\n",
    "\n",
    "- Feature Engineering: Creates comprehensive time-series aggregations\n",
    "\n",
    "- Custom Metrics: Implements the competition-specific AMEX metric\n",
    "\n",
    "- Cross-Validation: Uses 5-fold CV with proper model ensembling\n",
    "\n",
    "- Scalable Inference: Processes large test datasets in manageable chunks\n",
    "\n",
    "The solution achieves strong performance through careful optimization of both computation and memory usage, making it suitable for large-scale tabular data competitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b500eb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-02T17:50:46.506982Z",
     "iopub.status.busy": "2022-06-02T17:50:46.506197Z",
     "iopub.status.idle": "2022-06-02T17:50:50.266043Z",
     "shell.execute_reply": "2022-06-02T17:50:50.263778Z"
    },
    "papermill": {
     "duration": 3.768883,
     "end_time": "2022-06-02T17:50:50.268517",
     "exception": false,
     "start_time": "2022-06-02T17:50:46.499634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAPIDS version 21.10.01\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries for GPU-accelerated data processing and machine learning\n",
    "import pandas as pd, numpy as np  # Standard data manipulation libraries\n",
    "import cupy, cudf                # RAPIDS libraries for GPU acceleration\n",
    "import matplotlib.pyplot as plt, gc, os  # Plotting, garbage collection, and OS utilities\n",
    "\n",
    "# Display RAPIDS version for reproducibility\n",
    "print('RAPIDS version', cudf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e1c23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:50:50.279579Z",
     "iopub.status.busy": "2022-06-02T17:50:50.279240Z",
     "iopub.status.idle": "2022-06-02T17:50:50.283944Z",
     "shell.execute_reply": "2022-06-02T17:50:50.283212Z"
    },
    "papermill": {
     "duration": 0.011745,
     "end_time": "2022-06-02T17:50:50.285614",
     "exception": false,
     "start_time": "2022-06-02T17:50:50.273869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model version for tracking experiments\n",
    "VER = 1\n",
    "\n",
    "# Random seed for reproducible results across runs\n",
    "SEED = 17\n",
    "\n",
    "# Value to replace NaN/missing values (chosen to be outside typical range)\n",
    "NAN_VALUE = -127\n",
    "\n",
    "# Number of cross-validation folds for model validation\n",
    "FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec7285",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:50:50.294982Z",
     "iopub.status.busy": "2022-06-02T17:50:50.294611Z",
     "iopub.status.idle": "2022-06-02T17:51:12.388136Z",
     "shell.execute_reply": "2022-06-02T17:51:12.386321Z"
    },
    "papermill": {
     "duration": 22.100234,
     "end_time": "2022-06-02T17:51:12.389995",
     "exception": false,
     "start_time": "2022-06-02T17:50:50.289761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data...\n",
      "shape of data: (5531451, 190)\n"
     ]
    }
   ],
   "source": [
    "def read_file(path = '', usecols = None):\n",
    "    \"\"\"\n",
    "    Load parquet data with preprocessing for AMEX competition format\n",
    "    \n",
    "    Args:\n",
    "        path: Path to parquet file\n",
    "        usecols: Specific columns to load (memory optimization)\n",
    "    \n",
    "    Returns:\n",
    "        cuDF DataFrame with preprocessed data\n",
    "    \"\"\"\n",
    "    # Load parquet file (optionally with column selection for memory efficiency)\n",
    "    if usecols is not None:\n",
    "        df = cudf.read_parquet(path, columns = usecols)\n",
    "    else:\n",
    "        df = cudf.read_parquet(path)\n",
    "    \n",
    "    # Convert customer_ID from hexadecimal string to int64 for efficient processing\n",
    "    # Takes last 16 characters of hex string and converts to integer\n",
    "    df['customer_ID'] = df['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    \n",
    "    # Convert S_2 column to datetime format for temporal features\n",
    "    df.S_2 = cudf.to_datetime(df.S_2)\n",
    "    \n",
    "    # Replace all NaN values with predefined constant for XGBoost compatibility\n",
    "    df = df.fillna(NAN_VALUE)\n",
    "    print('shape of data:', df.shape)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load training data using the preprocessing function\n",
    "print('Reading train data...')\n",
    "TRAIN_PATH = '../input/amex-data-integer-dtypes-parquet-format/train.parquet'\n",
    "train = read_file(path = TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc51e31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:51:12.399951Z",
     "iopub.status.busy": "2022-06-02T17:51:12.399663Z",
     "iopub.status.idle": "2022-06-02T17:51:12.626223Z",
     "shell.execute_reply": "2022-06-02T17:51:12.625540Z"
    },
    "papermill": {
     "duration": 0.233399,
     "end_time": "2022-06-02T17:51:12.627996",
     "exception": false,
     "start_time": "2022-06-02T17:51:12.394597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>S_2</th>\n",
       "      <th>P_2</th>\n",
       "      <th>D_39</th>\n",
       "      <th>B_1</th>\n",
       "      <th>B_2</th>\n",
       "      <th>R_1</th>\n",
       "      <th>S_3</th>\n",
       "      <th>D_41</th>\n",
       "      <th>B_3</th>\n",
       "      <th>...</th>\n",
       "      <th>D_136</th>\n",
       "      <th>D_137</th>\n",
       "      <th>D_138</th>\n",
       "      <th>D_139</th>\n",
       "      <th>D_140</th>\n",
       "      <th>D_141</th>\n",
       "      <th>D_142</th>\n",
       "      <th>D_143</th>\n",
       "      <th>D_144</th>\n",
       "      <th>D_145</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4532153018459703766</td>\n",
       "      <td>2017-03-09</td>\n",
       "      <td>0.938469</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008724</td>\n",
       "      <td>1.006838</td>\n",
       "      <td>0.009228</td>\n",
       "      <td>0.124035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-127.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4532153018459703766</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>0.936665</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004923</td>\n",
       "      <td>1.000653</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>0.126750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-127.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005492</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4532153018459703766</td>\n",
       "      <td>2017-05-28</td>\n",
       "      <td>0.954180</td>\n",
       "      <td>3</td>\n",
       "      <td>0.021655</td>\n",
       "      <td>1.009672</td>\n",
       "      <td>0.006815</td>\n",
       "      <td>0.123977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009423</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-127.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4532153018459703766</td>\n",
       "      <td>2017-06-13</td>\n",
       "      <td>0.960384</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013683</td>\n",
       "      <td>1.002700</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.117169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-127.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4532153018459703766</td>\n",
       "      <td>2017-07-16</td>\n",
       "      <td>0.947248</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015193</td>\n",
       "      <td>1.000727</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0.117325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009312</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-127.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           customer_ID        S_2       P_2  D_39       B_1       B_2  \\\n",
       "0 -4532153018459703766 2017-03-09  0.938469     0  0.008724  1.006838   \n",
       "1 -4532153018459703766 2017-04-07  0.936665     0  0.004923  1.000653   \n",
       "2 -4532153018459703766 2017-05-28  0.954180     3  0.021655  1.009672   \n",
       "3 -4532153018459703766 2017-06-13  0.960384     0  0.013683  1.002700   \n",
       "4 -4532153018459703766 2017-07-16  0.947248     0  0.015193  1.000727   \n",
       "\n",
       "        R_1       S_3  D_41       B_3  ...  D_136  D_137  D_138  D_139  D_140  \\\n",
       "0  0.009228  0.124035   0.0  0.004709  ...     -1     -1     -1      0      0   \n",
       "1  0.006151  0.126750   0.0  0.002714  ...     -1     -1     -1      0      0   \n",
       "2  0.006815  0.123977   0.0  0.009423  ...     -1     -1     -1      0      0   \n",
       "3  0.001373  0.117169   0.0  0.005531  ...     -1     -1     -1      0      0   \n",
       "4  0.007605  0.117325   0.0  0.009312  ...     -1     -1     -1      0      0   \n",
       "\n",
       "   D_141  D_142  D_143     D_144  D_145  \n",
       "0    0.0 -127.0      0  0.000610      0  \n",
       "1    0.0 -127.0      0  0.005492      0  \n",
       "2    0.0 -127.0      0  0.006986      0  \n",
       "3    0.0 -127.0      0  0.006527      0  \n",
       "4    0.0 -127.0      0  0.008126      0  \n",
       "\n",
       "[5 rows x 190 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows to understand data structure and verify loading\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7acc2ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:51:12.638806Z",
     "iopub.status.busy": "2022-06-02T17:51:12.638545Z",
     "iopub.status.idle": "2022-06-02T17:51:13.789851Z",
     "shell.execute_reply": "2022-06-02T17:51:13.788308Z"
    },
    "papermill": {
     "duration": 1.159254,
     "end_time": "2022-06-02T17:51:13.792040",
     "exception": false,
     "start_time": "2022-06-02T17:51:12.632786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_Agg Columns: Index(['P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', 'P_2_last', 'D_39_mean',\n",
      "       'D_39_std', 'D_39_min', 'D_39_max', 'D_39_last',\n",
      "       ...\n",
      "       'D_144_mean', 'D_144_std', 'D_144_min', 'D_144_max', 'D_144_last',\n",
      "       'D_145_mean', 'D_145_std', 'D_145_min', 'D_145_max', 'D_145_last'],\n",
      "      dtype='object', length=885)\n",
      "Cat_Agg Columns: Index(['P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', 'P_2_last', 'D_39_mean',\n",
      "       'D_39_std', 'D_39_min', 'D_39_max', 'D_39_last',\n",
      "       ...\n",
      "       'D_144_mean', 'D_144_std', 'D_144_min', 'D_144_max', 'D_144_last',\n",
      "       'D_145_mean', 'D_145_std', 'D_145_min', 'D_145_max', 'D_145_last'],\n",
      "      dtype='object', length=885)\n",
      "Shape after engineering: (458913, 918)\n"
     ]
    }
   ],
   "source": [
    "def process_and_feature_engineer(df):\n",
    "    \"\"\"\n",
    "    Create aggregated features from time-series customer data\n",
    "    \n",
    "    The AMEX dataset contains multiple observations per customer over time.\n",
    "    This function aggregates these into customer-level features for modeling.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw time-series data with multiple rows per customer\n",
    "        \n",
    "    Returns:\n",
    "        Aggregated customer-level features\n",
    "    \"\"\"\n",
    "    # Get all columns except customer_ID and timestamp\n",
    "    all_cols = [col for col in list(df.columns) if not (col == 'cutomer_ID' or col == 'S_2')]\n",
    "    \n",
    "    # Define categorical features based on AMEX competition knowledge\n",
    "    cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
    "    \n",
    "    # Remaining features are numerical\n",
    "    num_features = [col for col in all_cols if col not in cat_features]\n",
    "    \n",
    "    # Create comprehensive aggregations for numerical features\n",
    "    # These capture different aspects of customer behavior over time\n",
    "    test_num_agg = df.groupby('customer_ID')[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    print('Num_Agg Columns:', test_num_agg.columns)\n",
    "    \n",
    "    # Create aggregations for categorical features\n",
    "    # Count: frequency of observations, Last: most recent value, Nunique: diversity\n",
    "    test_cat_agg = df.groupby('customer_ID')[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    print('Cat_Agg Columns:', test_num_agg.columns)  # Note: should be test_cat_agg.columns\n",
    "\n",
    "    # Combine numerical and categorical aggregations\n",
    "    df = cudf.concat([test_num_agg, test_cat_agg], axis = 1)\n",
    "    \n",
    "    # Clean up memory\n",
    "    del test_num_agg, test_cat_agg\n",
    "    print('Shape after engineering:', df.shape)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to training data\n",
    "train = process_and_feature_engineer(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea23a56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:51:13.802872Z",
     "iopub.status.busy": "2022-06-02T17:51:13.802591Z",
     "iopub.status.idle": "2022-06-02T17:51:14.757231Z",
     "shell.execute_reply": "2022-06-02T17:51:14.756438Z"
    },
    "papermill": {
     "duration": 0.961977,
     "end_time": "2022-06-02T17:51:14.759101",
     "exception": false,
     "start_time": "2022-06-02T17:51:13.797124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 918 features!\n"
     ]
    }
   ],
   "source": [
    "# Load target labels for supervised learning\n",
    "targets = cudf.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "\n",
    "# Convert customer_ID to match format used in features\n",
    "targets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "targets = targets.set_index('customer_ID')\n",
    "\n",
    "# Merge features with targets\n",
    "train = train.merge(targets, left_index = True, right_index = True, how = 'left')\n",
    "del targets\n",
    "\n",
    "# Reset index and define feature columns (all except customer_ID and target)\n",
    "train = train.reset_index()\n",
    "FEATURES = train.columns[1:-1]  # Exclude customer_ID (index 0) and target (last column)\n",
    "print(f'There are {len(FEATURES)} features!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7882e5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:51:14.769862Z",
     "iopub.status.busy": "2022-06-02T17:51:14.769582Z",
     "iopub.status.idle": "2022-06-02T17:51:14.868847Z",
     "shell.execute_reply": "2022-06-02T17:51:14.867962Z"
    },
    "papermill": {
     "duration": 0.107343,
     "end_time": "2022-06-02T17:51:14.871324",
     "exception": false,
     "start_time": "2022-06-02T17:51:14.763981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Version 1.6.1\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for modeling\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "print('XGB Version', xgb.__version__)\n",
    "\n",
    "# XGBoost hyperparameters optimized for AMEX competition\n",
    "xgb_params = {\n",
    "    'max_depth': 4,           # Tree depth - prevents overfitting\n",
    "    'learning_rate': 0.05,    # Learning rate - conservative for better generalization\n",
    "    'subsample': 0.8,         # Row sampling - reduces overfitting\n",
    "    'colsample_bytree': 0.6,  # Column sampling - reduces overfitting\n",
    "    'eval_metric': 'logloss', # Evaluation metric during training\n",
    "    'objective': 'binary:logistic',  # Binary classification objective\n",
    "    'tree_method': 'gpu_hist',       # GPU acceleration for faster training\n",
    "    'predictor': 'gpu_predictor',    # GPU acceleration for prediction\n",
    "    'random_state': SEED      # Reproducible results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcaed05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:51:14.882815Z",
     "iopub.status.busy": "2022-06-02T17:51:14.882550Z",
     "iopub.status.idle": "2022-06-02T17:51:14.890637Z",
     "shell.execute_reply": "2022-06-02T17:51:14.889962Z"
    },
    "papermill": {
     "duration": 0.015257,
     "end_time": "2022-06-02T17:51:14.892303",
     "exception": false,
     "start_time": "2022-06-02T17:51:14.877046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IterLoadForDMatrix(xgb.core.DataIter):\n",
    "    \"\"\"\n",
    "    Custom iterator for memory-efficient XGBoost training with large datasets\n",
    "    \n",
    "    Loads data in batches to avoid memory overflow while maintaining GPU acceleration\n",
    "    \"\"\"\n",
    "    def __init__(self, df = None, features = None, target = None, batch_size = 256 * 1024):\n",
    "        self.features = features    # Feature column names\n",
    "        self.target = target       # Target column name\n",
    "        self.df = df              # Source DataFrame\n",
    "        self.it = 0               # Current iteration counter\n",
    "        self.batch_size = batch_size    # Rows per batch\n",
    "        self.batches = int(np.ceil(len(df) / self.batch_size))  # Total batches needed\n",
    "        super().__init__()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset iterator to beginning\"\"\"\n",
    "        self.it = 0\n",
    "    \n",
    "    def next(self, input_data):\n",
    "        \"\"\"\n",
    "        Load next batch of data\n",
    "        \n",
    "        Returns:\n",
    "            1 if data loaded successfully, 0 if no more data\n",
    "        \"\"\"\n",
    "        if self.it == self.batches:\n",
    "            return 0  # No more batches\n",
    "        \n",
    "        # Calculate batch boundaries\n",
    "        a = self.it * self.batch_size\n",
    "        b = min((self.it + 1) * self.batch_size, len(self.df))\n",
    "        \n",
    "        # Load batch as cuDF DataFrame\n",
    "        dt = cudf.DataFrame(self.df.iloc[a:b])\n",
    "        \n",
    "        # Pass data to XGBoost\n",
    "        input_data(data = dt[self.features], label = dt[self.target])\n",
    "        \n",
    "        self.it += 1\n",
    "        return 1  # Successfully loaded batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669bd5af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:51:14.903166Z",
     "iopub.status.busy": "2022-06-02T17:51:14.902591Z",
     "iopub.status.idle": "2022-06-02T17:51:14.911284Z",
     "shell.execute_reply": "2022-06-02T17:51:14.910560Z"
    },
    "papermill": {
     "duration": 0.01572,
     "end_time": "2022-06-02T17:51:14.912908",
     "exception": false,
     "start_time": "2022-06-02T17:51:14.897188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def amex_metric_mod(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    AMEX competition metric: weighted combination of Gini coefficient and top-4% precision\n",
    "    \n",
    "    This metric balances overall ranking quality (Gini) with performance on highest-risk customers (top-4%)\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth binary labels\n",
    "        y_pred: Predicted probabilities\n",
    "        \n",
    "    Returns:\n",
    "        AMEX metric score (higher is better)\n",
    "    \"\"\"\n",
    "    # Calculate top-4% precision with class weighting\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]  # Sort by prediction descending\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)     # Weight negative class 20x\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]  # Top 4% by weight\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])  # Precision in top 4%\n",
    "\n",
    "    # Calculate normalized Gini coefficient\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:  # Calculate for both prediction and random\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] * weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    # Return weighted combination: 50% normalized Gini + 50% top-4% precision\n",
    "    return 0.5 * (gini/gini + top_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a1b31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:51:14.923352Z",
     "iopub.status.busy": "2022-06-02T17:51:14.923087Z",
     "iopub.status.idle": "2022-06-02T17:59:45.642474Z",
     "shell.execute_reply": "2022-06-02T17:59:45.641614Z"
    },
    "papermill": {
     "duration": 510.726595,
     "end_time": "2022-06-02T17:59:45.644316",
     "exception": false,
     "start_time": "2022-06-02T17:51:14.917721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "### Train size 367130 Valid size 91783\n",
      "### Training with 100% fold data...\n",
      "#########################\n",
      "[0]\tdtrain-logloss:0.66203\tdvalid-logloss:0.66213\n",
      "[100]\tdtrain-logloss:0.23650\tdvalid-logloss:0.23997\n",
      "[200]\tdtrain-logloss:0.22231\tdvalid-logloss:0.22796\n",
      "[300]\tdtrain-logloss:0.21624\tdvalid-logloss:0.22392\n",
      "[400]\tdtrain-logloss:0.21226\tdvalid-logloss:0.22194\n",
      "[500]\tdtrain-logloss:0.20908\tdvalid-logloss:0.22082\n",
      "[600]\tdtrain-logloss:0.20638\tdvalid-logloss:0.22003\n",
      "[700]\tdtrain-logloss:0.20386\tdvalid-logloss:0.21946\n",
      "[800]\tdtrain-logloss:0.20155\tdvalid-logloss:0.21910\n",
      "[900]\tdtrain-logloss:0.19940\tdvalid-logloss:0.21873\n",
      "[1000]\tdtrain-logloss:0.19728\tdvalid-logloss:0.21853\n",
      "[1100]\tdtrain-logloss:0.19531\tdvalid-logloss:0.21835\n",
      "[1200]\tdtrain-logloss:0.19338\tdvalid-logloss:0.21819\n",
      "[1300]\tdtrain-logloss:0.19148\tdvalid-logloss:0.21809\n",
      "[1400]\tdtrain-logloss:0.18963\tdvalid-logloss:0.21795\n",
      "[1500]\tdtrain-logloss:0.18784\tdvalid-logloss:0.21783\n",
      "[1600]\tdtrain-logloss:0.18604\tdvalid-logloss:0.21783\n",
      "Kaggle Metric = 0.7899038927901583 \n",
      "\n",
      "#########################\n",
      "### Fold 2\n",
      "### Train size 367130 Valid size 91783\n",
      "### Training with 100% fold data...\n",
      "#########################\n",
      "[0]\tdtrain-logloss:0.66203\tdvalid-logloss:0.66208\n",
      "[100]\tdtrain-logloss:0.23652\tdvalid-logloss:0.24039\n",
      "[200]\tdtrain-logloss:0.22228\tdvalid-logloss:0.22852\n",
      "[300]\tdtrain-logloss:0.21618\tdvalid-logloss:0.22463\n",
      "[400]\tdtrain-logloss:0.21215\tdvalid-logloss:0.22265\n",
      "[500]\tdtrain-logloss:0.20894\tdvalid-logloss:0.22140\n",
      "[600]\tdtrain-logloss:0.20627\tdvalid-logloss:0.22059\n",
      "[700]\tdtrain-logloss:0.20383\tdvalid-logloss:0.22002\n",
      "[800]\tdtrain-logloss:0.20157\tdvalid-logloss:0.21964\n",
      "[900]\tdtrain-logloss:0.19938\tdvalid-logloss:0.21935\n",
      "[1000]\tdtrain-logloss:0.19724\tdvalid-logloss:0.21912\n",
      "[1100]\tdtrain-logloss:0.19520\tdvalid-logloss:0.21896\n",
      "[1200]\tdtrain-logloss:0.19322\tdvalid-logloss:0.21887\n",
      "[1300]\tdtrain-logloss:0.19130\tdvalid-logloss:0.21870\n",
      "[1400]\tdtrain-logloss:0.18950\tdvalid-logloss:0.21865\n",
      "[1500]\tdtrain-logloss:0.18769\tdvalid-logloss:0.21856\n",
      "[1557]\tdtrain-logloss:0.18668\tdvalid-logloss:0.21854\n",
      "Kaggle Metric = 0.7910654048212296 \n",
      "\n",
      "#########################\n",
      "### Fold 3\n",
      "### Train size 367130 Valid size 91783\n",
      "### Training with 100% fold data...\n",
      "#########################\n",
      "[0]\tdtrain-logloss:0.66209\tdvalid-logloss:0.66199\n",
      "[100]\tdtrain-logloss:0.23711\tdvalid-logloss:0.23790\n",
      "[200]\tdtrain-logloss:0.22285\tdvalid-logloss:0.22607\n",
      "[300]\tdtrain-logloss:0.21671\tdvalid-logloss:0.22225\n",
      "[400]\tdtrain-logloss:0.21255\tdvalid-logloss:0.22035\n",
      "[500]\tdtrain-logloss:0.20934\tdvalid-logloss:0.21917\n",
      "[600]\tdtrain-logloss:0.20670\tdvalid-logloss:0.21855\n",
      "[700]\tdtrain-logloss:0.20429\tdvalid-logloss:0.21807\n",
      "[800]\tdtrain-logloss:0.20195\tdvalid-logloss:0.21772\n",
      "[900]\tdtrain-logloss:0.19985\tdvalid-logloss:0.21745\n",
      "[1000]\tdtrain-logloss:0.19772\tdvalid-logloss:0.21721\n",
      "[1100]\tdtrain-logloss:0.19566\tdvalid-logloss:0.21707\n",
      "[1200]\tdtrain-logloss:0.19369\tdvalid-logloss:0.21684\n",
      "[1300]\tdtrain-logloss:0.19177\tdvalid-logloss:0.21675\n",
      "[1400]\tdtrain-logloss:0.18990\tdvalid-logloss:0.21667\n",
      "[1500]\tdtrain-logloss:0.18810\tdvalid-logloss:0.21670\n",
      "[1507]\tdtrain-logloss:0.18797\tdvalid-logloss:0.21672\n",
      "Kaggle Metric = 0.7926388673353528 \n",
      "\n",
      "#########################\n",
      "### Fold 4\n",
      "### Train size 367131 Valid size 91782\n",
      "### Training with 100% fold data...\n",
      "#########################\n",
      "[0]\tdtrain-logloss:0.66202\tdvalid-logloss:0.66213\n",
      "[100]\tdtrain-logloss:0.23668\tdvalid-logloss:0.24002\n",
      "[200]\tdtrain-logloss:0.22246\tdvalid-logloss:0.22796\n",
      "[300]\tdtrain-logloss:0.21619\tdvalid-logloss:0.22388\n",
      "[400]\tdtrain-logloss:0.21215\tdvalid-logloss:0.22210\n",
      "[500]\tdtrain-logloss:0.20891\tdvalid-logloss:0.22090\n",
      "[600]\tdtrain-logloss:0.20610\tdvalid-logloss:0.22022\n",
      "[700]\tdtrain-logloss:0.20366\tdvalid-logloss:0.21982\n",
      "[800]\tdtrain-logloss:0.20135\tdvalid-logloss:0.21944\n",
      "[900]\tdtrain-logloss:0.19910\tdvalid-logloss:0.21923\n",
      "[1000]\tdtrain-logloss:0.19701\tdvalid-logloss:0.21911\n",
      "[1100]\tdtrain-logloss:0.19501\tdvalid-logloss:0.21905\n",
      "[1200]\tdtrain-logloss:0.19306\tdvalid-logloss:0.21891\n",
      "[1300]\tdtrain-logloss:0.19119\tdvalid-logloss:0.21882\n",
      "[1400]\tdtrain-logloss:0.18944\tdvalid-logloss:0.21881\n",
      "[1448]\tdtrain-logloss:0.18856\tdvalid-logloss:0.21881\n",
      "Kaggle Metric = 0.7896059867023926 \n",
      "\n",
      "#########################\n",
      "### Fold 5\n",
      "### Train size 367131 Valid size 91782\n",
      "### Training with 100% fold data...\n",
      "#########################\n",
      "[0]\tdtrain-logloss:0.66208\tdvalid-logloss:0.66205\n",
      "[100]\tdtrain-logloss:0.23708\tdvalid-logloss:0.23774\n",
      "[200]\tdtrain-logloss:0.22282\tdvalid-logloss:0.22562\n",
      "[300]\tdtrain-logloss:0.21662\tdvalid-logloss:0.22161\n",
      "[400]\tdtrain-logloss:0.21265\tdvalid-logloss:0.21977\n",
      "[500]\tdtrain-logloss:0.20940\tdvalid-logloss:0.21858\n",
      "[600]\tdtrain-logloss:0.20667\tdvalid-logloss:0.21782\n",
      "[700]\tdtrain-logloss:0.20420\tdvalid-logloss:0.21727\n",
      "[800]\tdtrain-logloss:0.20189\tdvalid-logloss:0.21686\n",
      "[900]\tdtrain-logloss:0.19972\tdvalid-logloss:0.21659\n",
      "[1000]\tdtrain-logloss:0.19766\tdvalid-logloss:0.21639\n",
      "[1100]\tdtrain-logloss:0.19563\tdvalid-logloss:0.21617\n",
      "[1200]\tdtrain-logloss:0.19371\tdvalid-logloss:0.21607\n",
      "[1300]\tdtrain-logloss:0.19180\tdvalid-logloss:0.21594\n",
      "[1400]\tdtrain-logloss:0.18993\tdvalid-logloss:0.21587\n",
      "[1500]\tdtrain-logloss:0.18814\tdvalid-logloss:0.21583\n",
      "[1600]\tdtrain-logloss:0.18635\tdvalid-logloss:0.21572\n",
      "[1700]\tdtrain-logloss:0.18457\tdvalid-logloss:0.21569\n",
      "[1800]\tdtrain-logloss:0.18282\tdvalid-logloss:0.21565\n",
      "[1900]\tdtrain-logloss:0.18111\tdvalid-logloss:0.21562\n",
      "[1987]\tdtrain-logloss:0.17966\tdvalid-logloss:0.21562\n",
      "Kaggle Metric = 0.7964284193981604 \n",
      "\n",
      "#########################\n",
      "OVERALL CV Kaggle Metric = 0.7919123655546447\n"
     ]
    }
   ],
   "source": [
    "# Initialize containers for model outputs\n",
    "importances = []                    # Feature importance from each fold\n",
    "oof = np.zeros(len(train))         # Out-of-fold predictions\n",
    "train = train.to_pandas()          # Convert to pandas for sklearn compatibility\n",
    "TRAIN_SUBSAMPLE = 1.0              # Use full dataset (can be reduced for testing)\n",
    "gc.collect()                       # Clean memory before training\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "skf = KFold(n_splits = FOLDS)\n",
    "\n",
    "# Train model on each fold\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train.target)):\n",
    "    \n",
    "    # Optional subsampling for faster experimentation\n",
    "    if TRAIN_SUBSAMPLE < 1.0:\n",
    "        np.random.seed(SEED)\n",
    "        train_idx = np.random.choice(train_idx, int(len(train_idx)*TRAIN_SUBSAMPLE), replace = False)\n",
    "        np.random.seed(None)\n",
    "    \n",
    "    print('#' * 25)\n",
    "    print('### Fold', fold + 1)\n",
    "    print('### Train size', len(train_idx), 'Valid size', len(valid_idx))\n",
    "    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n",
    "    print('#' * 25)\n",
    "    \n",
    "    # Prepare training data with memory-efficient iterator\n",
    "    Xy_train = IterLoadForDMatrix(train.loc[train_idx], FEATURES, 'target')\n",
    "    \n",
    "    # Prepare validation data\n",
    "    X_valid = train.loc[valid_idx, FEATURES]\n",
    "    y_valid = train.loc[valid_idx, 'target']\n",
    "    \n",
    "    # Create XGBoost data structures\n",
    "    dtrain = xgb.DeviceQuantileDMatrix(Xy_train, max_bin = 256)  # GPU-optimized training matrix\n",
    "    dvalid = xgb.DMatrix(data = X_valid, label = y_valid)        # Validation matrix\n",
    "    \n",
    "    # Train XGBoost model with early stopping\n",
    "    model = xgb.train(xgb_params,\n",
    "                      dtrain = dtrain,\n",
    "                      evals = [(dtrain, 'dtrain'), (dvalid, 'dvalid')],\n",
    "                      num_boost_round = 9999,      # Large number with early stopping\n",
    "                      early_stopping_rounds = 100, # Stop if no improvement for 100 rounds\n",
    "                      verbose_eval = 100           # Print progress every 100 rounds\n",
    "                     )\n",
    "    # Save trained model for later inference\n",
    "    model.save_model(f'XGB_v{VER}_fold{fold}.xgb')\n",
    "    \n",
    "    # Extract and store feature importances\n",
    "    dd = model.get_score(importance_type = 'weight')\n",
    "    df = pd.DataFrame({'feature': dd.keys(), f'importance_{fold}': dd.values()})\n",
    "    importances.append(df)\n",
    "    \n",
    "    # Generate out-of-fold predictions and calculate metric\n",
    "    oof_preds = model.predict(dvalid)\n",
    "    acc = amex_metric_mod(y_valid.values, oof_preds)\n",
    "    print('Kaggle Metric =', acc, '\\\\n')\n",
    "    \n",
    "    # Store out-of-fold predictions for final CV score\n",
    "    oof[valid_idx] = oof_preds\n",
    "    \n",
    "    # Clean up memory after each fold\n",
    "    del dtrain, Xy_train, dd, df\n",
    "    del X_valid, y_valid, dvalid, model\n",
    "    _ = gc.collect()\n",
    "\n",
    "# Calculate overall cross-validation score\n",
    "print('#' * 25)\n",
    "acc = amex_metric_mod(train.target.values, oof)\n",
    "print('OVERALL CV Kaggle Metric =', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6387331b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:59:45.666700Z",
     "iopub.status.busy": "2022-06-02T17:59:45.666039Z",
     "iopub.status.idle": "2022-06-02T17:59:45.674841Z",
     "shell.execute_reply": "2022-06-02T17:59:45.674034Z"
    },
    "papermill": {
     "duration": 0.021951,
     "end_time": "2022-06-02T17:59:45.676587",
     "exception": false,
     "start_time": "2022-06-02T17:59:45.654636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove training data and trigger garbage collection to free memory for inference\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfedb19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:59:45.700015Z",
     "iopub.status.busy": "2022-06-02T17:59:45.699696Z",
     "iopub.status.idle": "2022-06-02T17:59:48.467590Z",
     "shell.execute_reply": "2022-06-02T17:59:48.466313Z"
    },
    "papermill": {
     "duration": 2.781916,
     "end_time": "2022-06-02T17:59:48.469414",
     "exception": false,
     "start_time": "2022-06-02T17:59:45.687498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test data...\n",
      "shape of data: (11363762, 2)\n",
      "We will process test data as 4 separate parts.\n",
      "There will be 231155 customers in each part (except the last part).\n",
      "Below are number of rows in each part:\n",
      "[2841209, 2839857, 2842105, 2840591]\n"
     ]
    }
   ],
   "source": [
    "# CALCULATE SIZE OF EACH SEPARATE TEST PART\n",
    "def get_rows(customers, test, NUM_PARTS = 4, verbose = ''):\n",
    "    \"\"\"\n",
    "    Calculate optimal chunking strategy for processing large test dataset\n",
    "    \n",
    "    Args:\n",
    "        customers: List of unique customer IDs\n",
    "        test: Test dataset (used for row counting)\n",
    "        NUM_PARTS: Number of chunks to split data into\n",
    "        verbose: Description for logging\n",
    "        \n",
    "    Returns:\n",
    "        rows: List of row counts per chunk\n",
    "        chunk: Number of customers per chunk\n",
    "    \"\"\"\n",
    "    chunk = len(customers)//NUM_PARTS  # Customers per chunk\n",
    "    \n",
    "    if verbose != '':\n",
    "        print(f'We will process {verbose} data as {NUM_PARTS} separate parts.')\n",
    "        print(f'There will be {chunk} customers in each part (except the last part).')\n",
    "        print('Below are number of rows in each part:')\n",
    "    \n",
    "    rows = []\n",
    "    # Calculate rows needed for each chunk of customers\n",
    "    for k in range(NUM_PARTS):\n",
    "        if k==NUM_PARTS-1: \n",
    "            cc = customers[k*chunk:]  # Last chunk gets remaining customers\n",
    "        else: \n",
    "            cc = customers[k*chunk:(k+1)*chunk]  # Regular chunk\n",
    "        \n",
    "        # Count rows for this set of customers\n",
    "        s = test.loc[test.customer_ID.isin(cc)].shape\n",
    "        rows.append(s)\n",
    "    \n",
    "    if verbose != '': \n",
    "        print( rows )\n",
    "    return rows, chunk\n",
    "\n",
    "# COMPUTE SIZE OF 4 PARTS FOR TEST DATA\n",
    "NUM_PARTS = 4\n",
    "TEST_PATH = '../input/amex-data-integer-dtypes-parquet-format/test.parquet'\n",
    "\n",
    "print(f'Reading test data...')\n",
    "# Load only customer_ID and timestamp columns for chunking calculation\n",
    "test = read_file(path = TEST_PATH, usecols = ['customer_ID','S_2'])\n",
    "customers = test[['customer_ID']].drop_duplicates().sort_index().values.flatten()\n",
    "rows, num_cust = get_rows(customers, test[['customer_ID']], NUM_PARTS = NUM_PARTS, verbose = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5790aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:59:48.492772Z",
     "iopub.status.busy": "2022-06-02T17:59:48.492478Z",
     "iopub.status.idle": "2022-06-02T18:02:25.875256Z",
     "shell.execute_reply": "2022-06-02T18:02:25.874475Z"
    },
    "papermill": {
     "duration": 157.397618,
     "end_time": "2022-06-02T18:02:25.877442",
     "exception": false,
     "start_time": "2022-06-02T17:59:48.479824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading test data...\n",
      "shape of data: (11363762, 190)\n",
      "=> Test part 1 has shape (2841209, 190)\n",
      "Num_Agg Columns: Index(['P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', 'P_2_last', 'D_39_mean',\n",
      "       'D_39_std', 'D_39_min', 'D_39_max', 'D_39_last',\n",
      "       ...\n",
      "       'D_144_mean', 'D_144_std', 'D_144_min', 'D_144_max', 'D_144_last',\n",
      "       'D_145_mean', 'D_145_std', 'D_145_min', 'D_145_max', 'D_145_last'],\n",
      "      dtype='object', length=885)\n",
      "Cat_Agg Columns: Index(['P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', 'P_2_last', 'D_39_mean',\n",
      "       'D_39_std', 'D_39_min', 'D_39_max', 'D_39_last',\n",
      "       ...\n",
      "       'D_144_mean', 'D_144_std', 'D_144_min', 'D_144_max', 'D_144_last',\n",
      "       'D_145_mean', 'D_145_std', 'D_145_min', 'D_145_max', 'D_145_last'],\n",
      "      dtype='object', length=885)\n",
      "Shape after engineering: (231155, 918)\n",
      "\n",
      "Reading test data...\n",
      "shape of data: (11363762, 190)\n",
      "=> Test part 2 has shape (2839857, 190)\n",
      "Num_Agg Columns: Index(['P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', 'P_2_last', 'D_39_mean',\n",
      "       'D_39_std', 'D_39_min', 'D_39_max', 'D_39_last',\n",
      "       ...\n",
      "       'D_144_mean', 'D_144_std', 'D_144_min', 'D_144_max', 'D_144_last',\n",
      "       'D_145_mean', 'D_145_std', 'D_145_min', 'D_145_max', 'D_145_last'],\n",
      "      dtype='object', length=885)\n",
      "Cat_Agg Columns: Index(['P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', 'P_2_last', 'D_39_mean',\n",
      "       'D_39_std', 'D_39_min', 'D_39_max', 'D_39_last',\n",
      "       ...\n",
      "       'D_144_mean', 'D_144_std', 'D_144_min', 'D_144_max', 'D_144_last',\n",
      "       'D_145_mean', 'D_145_std', 'D_145_min', 'D_145_max', 'D_145_last'],\n",
      "      dtype='object', length=885)\n",
      "Shape after engineering: (231155, 918)\n",
      "\n",
      "Reading test data...\n",
      "shape of data: (11363762, 190)\n",
      "=> Test part 3 has shape (2842105, 190)\n",
      "Num_Agg Columns: Index(['P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', 'P_2_last', 'D_39_mean',\n",
      "       'D_39_std', 'D_39_min', 'D_39_max', 'D_39_last',\n",
      "       ...\n",
      "       'D_144_mean', 'D_144_std', 'D_144_min', 'D_144_max', 'D_144_last',\n",
      "       'D_145_mean', 'D_145_std', 'D_145_min', 'D_145_max', 'D_145_last'],\n",
      "      dtype='object', length=885)\n",
      "Cat_Agg Columns: Index(['P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', 'P_2_last', 'D_39_mean',\n",
      "       'D_39_std', 'D_39_min', 'D_39_max', 'D_39_last',\n",
      "       ...\n",
      "       'D_144_mean', 'D_144_std', 'D_144_min', 'D_144_max', 'D_144_last',\n",
      "       'D_145_mean', 'D_145_std', 'D_145_min', 'D_145_max', 'D_145_last'],\n",
      "      dtype='object', length=885)\n",
      "Shape after engineering: (231155, 918)\n",
      "\n",
      "Reading test data...\n",
      "shape of data: (11363762, 190)\n",
      "=> Test part 4 has shape (2840591, 190)\n",
      "Num_Agg Columns: Index(['P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', 'P_2_last', 'D_39_mean',\n",
      "       'D_39_std', 'D_39_min', 'D_39_max', 'D_39_last',\n",
      "       ...\n",
      "       'D_144_mean', 'D_144_std', 'D_144_min', 'D_144_max', 'D_144_last',\n",
      "       'D_145_mean', 'D_145_std', 'D_145_min', 'D_145_max', 'D_145_last'],\n",
      "      dtype='object', length=885)\n",
      "Cat_Agg Columns: Index(['P_2_mean', 'P_2_std', 'P_2_min', 'P_2_max', 'P_2_last', 'D_39_mean',\n",
      "       'D_39_std', 'D_39_min', 'D_39_max', 'D_39_last',\n",
      "       ...\n",
      "       'D_144_mean', 'D_144_std', 'D_144_min', 'D_144_max', 'D_144_last',\n",
      "       'D_145_mean', 'D_145_std', 'D_145_min', 'D_145_max', 'D_145_last'],\n",
      "      dtype='object', length=885)\n",
      "Shape after engineering: (231156, 918)\n"
     ]
    }
   ],
   "source": [
    "# INFER TEST DATA IN PARTS\n",
    "skip_rows = 0      # Track position in full dataset\n",
    "skip_cust = 0      # Track position in customer list\n",
    "test_preds = []    # Store predictions from each chunk\n",
    "\n",
    "# Process each chunk of test data\n",
    "for k in range(NUM_PARTS):\n",
    "    \n",
    "    # READ PART OF TEST DATA\n",
    "    print(f'\\\\nReading test data...')\n",
    "    test = read_file(path = TEST_PATH)  # Load full test data\n",
    "    test = test.iloc[skip_rows:skip_rows+rows[k]]  # Extract current chunk\n",
    "    skip_rows += rows[k]  # Update position for next chunk\n",
    "    print(f'=> Test part {k+1} has shape', test.shape )\n",
    "    \n",
    "    # PROCESS AND FEATURE ENGINEER PART OF TEST DATA\n",
    "    test = process_and_feature_engineer(test)  # Apply same feature engineering as training\n",
    "    \n",
    "    # Select customers for this chunk\n",
    "    if k==NUM_PARTS-1: \n",
    "        test = test.loc[customers[skip_cust:]]  # Last chunk gets remaining customers\n",
    "    else: \n",
    "        test = test.loc[customers[skip_cust:skip_cust+num_cust]]  # Regular chunk\n",
    "    skip_cust += num_cust  # Update position for next chunk\n",
    "    \n",
    "    # TEST DATA FOR XGB\n",
    "    X_test = test[FEATURES]  # Extract features for prediction\n",
    "    dtest = xgb.DMatrix(data=X_test)  # Create XGBoost data structure\n",
    "    test = test[['P_2_mean']]  # Keep only one column to reduce memory usage\n",
    "    del X_test\n",
    "    gc.collect()\n",
    "\n",
    "    # INFER XGB MODELS ON TEST DATA\n",
    "    # Load first model and make prediction\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(f'XGB_v{VER}_fold0.xgb')\n",
    "    preds = model.predict(dtest)\n",
    "    \n",
    "    # Add predictions from remaining folds\n",
    "    for f in range(1,FOLDS):\n",
    "        model.load_model(f'XGB_v{VER}_fold{f}.xgb')\n",
    "        preds += model.predict(dtest)\n",
    "    \n",
    "    # Average predictions across all folds\n",
    "    preds /= FOLDS\n",
    "    test_preds.append(preds)\n",
    "\n",
    "    # CLEAN MEMORY\n",
    "    del dtest, model\n",
    "    _ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79712964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T18:02:25.900971Z",
     "iopub.status.busy": "2022-06-02T18:02:25.900679Z",
     "iopub.status.idle": "2022-06-02T18:02:26.980933Z",
     "shell.execute_reply": "2022-06-02T18:02:26.979980Z"
    },
    "papermill": {
     "duration": 1.093905,
     "end_time": "2022-06-02T18:02:26.982890",
     "exception": false,
     "start_time": "2022-06-02T18:02:25.888985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file shape is (924621, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0359e97c244bbbbe2db7c21e891debe80e82291f2e470e...</td>\n",
       "      <td>0.002059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>035b3479c9020483c00b7dac8f816759bb3aa6fdd8dfab...</td>\n",
       "      <td>0.000367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>035a556cc13aae13de7bdcc71c81a1ab27f586f2ddf50e...</td>\n",
       "      <td>0.002860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>035bca6744c2fe912b15a0bc6011f3ec679cbc7c60e049...</td>\n",
       "      <td>0.039637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0359f31145b54da7258ed5ff894cbe50dd4302d3d4a1e9...</td>\n",
       "      <td>0.036869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_ID  prediction\n",
       "0  0359e97c244bbbbe2db7c21e891debe80e82291f2e470e...    0.002059\n",
       "1  035b3479c9020483c00b7dac8f816759bb3aa6fdd8dfab...    0.000367\n",
       "2  035a556cc13aae13de7bdcc71c81a1ab27f586f2ddf50e...    0.002860\n",
       "3  035bca6744c2fe912b15a0bc6011f3ec679cbc7c60e049...    0.039637\n",
       "4  0359f31145b54da7258ed5ff894cbe50dd4302d3d4a1e9...    0.036869"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE SUBMISSION FILE\n",
    "\n",
    "# Combine predictions from all test chunks\n",
    "test_preds = np.concatenate(test_preds)\n",
    "\n",
    "# Create DataFrame with customer IDs and predictions\n",
    "test = cudf.DataFrame(index=customers, data={'prediction':test_preds})\n",
    "\n",
    "# Load sample submission to get correct format and customer ID order\n",
    "sub = cudf.read_csv('../input/amex-default-prediction/sample_submission.csv')[['customer_ID']]\n",
    "\n",
    "# Convert customer IDs to match our internal format\n",
    "sub['customer_ID_hash'] = sub['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "sub = sub.set_index('customer_ID_hash')\n",
    "\n",
    "# Merge predictions with submission format\n",
    "sub = sub.merge(test[['prediction']], left_index=True, right_index=True, how='left')\n",
    "sub = sub.reset_index(drop=True)\n",
    "\n",
    "# Save submission file\n",
    "sub.to_csv(f'submission_xgb_v{VER}.csv', index=False)\n",
    "print('Submission file shape is', sub.shape )\n",
    "\n",
    "# Display first few predictions\n",
    "sub.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee20ff7c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 710.556958,
   "end_time": "2022-06-02T18:02:29.372441",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-02T17:50:38.815483",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
