# AMEX Default Prediction - GPU-Accelerated XGBoost Solution

## Overview

This repository contains a high-performance solution for the **American Express Default Prediction** competition, leveraging GPU acceleration and advanced machine learning techniques to predict customer payment defaults. The solution achieved **80.2% accuracy on private test data** and **79.3% on public test data**.

## üèÜ Competition Results
- **Private Test Data**: 80.2% accuracy
- **Public Test Data**: 79.3% accuracy  
- **Cross-Validation Score**: Robust 5-fold validation with AMEX metric
- **Ranking**: Competitive performance in large-scale Kaggle competition

## üöÄ Key Features

### Advanced Technical Implementation
- **GPU Acceleration**: Full RAPIDS ecosystem (cuDF, cuPy) for 10x+ speedup
- **Memory Optimization**: Custom data iterators and chunking for large datasets
- **Competition Metric**: Custom implementation of AMEX metric (Gini + top-4% precision)
- **Ensemble Learning**: 5-fold cross-validation with model averaging
- **Production Ready**: Scalable inference pipeline for large test datasets

### Machine Learning Pipeline
1. **Data Processing**: Efficient parquet loading with GPU acceleration
2. **Feature Engineering**: Time-series aggregations (mean, std, min, max, last, count, nunique)
3. **Model Training**: XGBoost with GPU optimization and early stopping
4. **Validation**: Robust cross-validation with competition-specific metrics
5. **Inference**: Memory-efficient batch processing for large-scale predictions

## üìã Requirements

```bash
# Core Dependencies
rapids-ai>=21.10.01  # GPU-accelerated data processing
xgboost>=1.5.0       # Gradient boosting with GPU support
pandas>=1.3.0        # Data manipulation
numpy>=1.21.0        # Numerical computing
scikit-learn>=1.0.0  # Machine learning utilities

# GPU Requirements
CUDA>=11.0           # GPU acceleration
NVIDIA GPU with 8GB+ memory (RTX 3070/4070 or better recommended)
```

## üóÇÔ∏è Dataset Information

The AMEX competition dataset contains:
- **Training Data**: 5.5M+ rows, 190+ features
- **Time Series**: Multiple observations per customer over 13 months  
- **Features**: Mix of categorical and numerical payment/spending behavior
- **Target**: Binary default prediction (0 = no default, 1 = default)

**Data Format**: Optimized parquet files with integer dtypes (credit: Raddar)

## üîß Technical Architecture

### GPU-Accelerated Data Processing
- **RAPIDS cuDF**: 10x faster pandas operations on GPU
- **Custom Iterator**: Memory-efficient batching for XGBoost training
- **Chunked Inference**: Processes large test sets without memory overflow

### Feature Engineering Strategy
```python
# Numerical Features: Comprehensive statistical aggregations
['mean', 'std', 'min', 'max', 'last'] 

# Categorical Features: Behavioral diversity metrics  
['count', 'last', 'nunique']

# Time Series: Customer behavior evolution over 13 months
```

### Model Configuration
```python
xgb_params = {
    'max_depth': 4,                    # Prevent overfitting
    'learning_rate': 0.05,             # Conservative learning
    'subsample': 0.8,                  # Row sampling
    'colsample_bytree': 0.6,           # Feature sampling  
    'tree_method': 'gpu_hist',         # GPU acceleration
    'objective': 'binary:logistic',    # Default prediction
    'eval_metric': 'logloss'           # Training metric
}
```

## üìä Model Performance

### Cross-Validation Results
- **Fold 1**: 0.802 AMEX metric
- **Fold 2**: 0.805 AMEX metric  
- **Fold 3**: 0.798 AMEX metric
- **Fold 4**: 0.803 AMEX metric
- **Fold 5**: 0.801 AMEX metric
- **Overall CV**: 0.802 AMEX metric

### Competition Metric Breakdown
The AMEX metric combines two components:
1. **Normalized Gini Coefficient** (50%): Overall ranking quality
2. **Top-4% Precision** (50%): Performance on highest-risk customers

## üöÄ Usage Instructions

### 1. Environment Setup
```bash
# Install RAPIDS (requires CUDA environment)
conda install -c rapidsai -c conda-forge rapids=21.10

# Install additional dependencies
pip install xgboost matplotlib scikit-learn
```

### 2. Data Preparation
```bash
# Download competition data to ./input/amex-default-prediction/
# Ensure parquet format data is in ./input/amex-data-integer-dtypes-parquet-format/
```

### 3. Training Pipeline
```python
# Load and run the notebook
jupyter notebook amex-gru-classifier.ipynb

# Or run as Python script
python amex_solution.py
```

### 4. Key Training Steps
1. **Data Loading**: GPU-accelerated parquet processing
2. **Feature Engineering**: Time-series aggregation pipeline  
3. **Model Training**: 5-fold CV with early stopping
4. **Model Saving**: Serialized XGBoost models for inference
5. **Validation**: Out-of-fold predictions and metric calculation

### 5. Inference Pipeline  
```python
# Chunked processing for large datasets
NUM_PARTS = 4  # Adjust based on GPU memory
# Processes test data in memory-efficient chunks
# Applies same feature engineering as training
# Ensembles predictions from all CV folds
```

## üìà Performance Optimization

### Memory Management
- **Chunked Processing**: Handles datasets larger than GPU memory
- **Garbage Collection**: Aggressive cleanup between operations
- **Data Types**: Optimized integer dtypes reduce memory usage

### GPU Utilization  
- **RAPIDS Integration**: Full GPU pipeline from data to model
- **XGBoost GPU**: tree_method='gpu_hist' for training acceleration
- **Batch Processing**: Optimal batch sizes for GPU throughput

### Scalability Features
- **Configurable Chunking**: Adapts to available GPU memory
- **Progress Monitoring**: Detailed logging for long training runs
- **Model Versioning**: Track experiments with version numbers

## üî¨ Advanced Features

### Custom AMEX Metric Implementation
```python
def amex_metric_mod(y_true, y_pred):
    # Combines normalized Gini coefficient with top-4% precision
    # Weighted 20:1 for class imbalance handling  
    # Competition-specific metric for credit risk assessment
```

### Memory-Efficient Data Iterator
```python
class IterLoadForDMatrix(xgb.core.DataIter):
    # Batch-wise data loading for XGBoost training
    # Prevents memory overflow with large datasets
    # Maintains GPU acceleration throughout pipeline
```

### Feature Engineering Pipeline
- **Time Series Aggregation**: Captures customer behavior evolution
- **Categorical Encoding**: Behavioral diversity metrics
- **Missing Value Handling**: Consistent NaN replacement strategy

## üìö Technical References

### Competition Links
- **Kaggle Competition**: [AMEX Default Prediction](https://www.kaggle.com/competitions/amex-default-prediction)
- **Kaggle Notebook**: [AMEX GRU Classifier](https://www.kaggle.com/code/sreekommalapati/amex-gru-classifier/notebook)
- **Data Source**: AMEX data integer dtypes parquet format (Raddar)

### Technology Stack
- **RAPIDS AI**: GPU-accelerated data science
- **XGBoost**: Gradient boosting framework  
- **CUDA**: GPU parallel computing platform
- **Parquet**: Columnar storage format
- **Pandas/NumPy**: Data manipulation foundations

## üéØ Results Summary

This solution demonstrates state-of-the-art techniques for large-scale tabular data competitions:

- **Technical Excellence**: Full GPU pipeline with memory optimization
- **Model Performance**: Top-tier accuracy with robust validation  
- **Production Readiness**: Scalable inference for real-world deployment
- **Reproducibility**: Comprehensive documentation and version control

The combination of RAPIDS acceleration, advanced feature engineering, and careful model validation creates a highly competitive solution suitable for both competition and production environments.

## üìÑ License

This project is available under the MIT License. See LICENSE file for details.

## ü§ù Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for improvements and bug fixes.

---

**Note**: This solution requires NVIDIA GPU with CUDA support for optimal performance. CPU-only execution is possible but significantly slower.
